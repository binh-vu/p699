{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np, glob, ujson\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2   \n",
    "\n",
    "# next cell\n",
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import optim\n",
    "from wavenet_vocoder.util import is_mulaw_quantize, is_scalar_input\n",
    "from wavenet_vocoder import WaveNet\n",
    "from wavenet_vocoder.data_loader import get_data_loader\n",
    "from wavenet_vocoder.train import train, train_one_step, MeanMetric\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from wavenet_vocoder.tfcompat.hparam import HParams\n",
    "\n",
    "hparams = HParams(\n",
    "    name=\"wavenet_vocoder\",\n",
    "\n",
    "    # Input type:\n",
    "    # 1. raw [-1, 1]\n",
    "    # 2. mulaw [-1, 1]\n",
    "    # 3. mulaw-quantize [0, mu]\n",
    "    # If input_type is raw or mulaw, network assumes scalar input and\n",
    "    # discretized mixture of logistic distributions output, otherwise one-hot\n",
    "    # input and softmax output are assumed.\n",
    "    # **NOTE**: if you change the one of the two parameters below, you need to\n",
    "    # re-run preprocessing before training.\n",
    "    input_type=\"mulaw-quantize\",\n",
    "    quantize_channels=256,  # 65536 or 256\n",
    "\n",
    "    # Audio:\n",
    "    # time-domain pre/post-processing\n",
    "    # e.g., preemphasis/inv_preemphasis\n",
    "    # ref: LPCNet https://arxiv.org/abs/1810.11846\n",
    "    preprocess=\"\",\n",
    "    postprocess=\"\",\n",
    "    # waveform domain scaling\n",
    "    global_gain_scale=1.0,\n",
    "\n",
    "    sample_rate=16000,\n",
    "    # this is only valid for mulaw is True\n",
    "    silence_threshold=2,\n",
    "    num_mels=80,\n",
    "    fmin=125,\n",
    "    fmax=7600,\n",
    "    fft_size=1024,\n",
    "    # shift can be specified by either hop_size or frame_shift_ms\n",
    "    hop_size=256,\n",
    "    frame_shift_ms=None,\n",
    "    win_length=1024,\n",
    "    win_length_ms=-1.0,\n",
    "    window=\"hann\",\n",
    "\n",
    "    # DC removal\n",
    "    highpass_cutoff=70.0,\n",
    "\n",
    "    # Parametric output distribution type for scalar input\n",
    "    # 1) Logistic or 2) Normal\n",
    "    output_distribution=\"Logistic\",\n",
    "    log_scale_min=-16.0,\n",
    "\n",
    "    # Model:\n",
    "    # This should equal to `quantize_channels` if mu-law quantize enabled\n",
    "    # otherwise num_mixture * 3 (pi, mean, log_scale)\n",
    "    # single mixture case: 2\n",
    "#     out_channels=10 * 3,\n",
    "    out_channels=256,\n",
    "    layers=24,\n",
    "    stacks=4,\n",
    "    residual_channels=128,\n",
    "    gate_channels=256,  # split into 2 gropus internally for gated activation\n",
    "    skip_out_channels=128,\n",
    "    dropout=0.0,\n",
    "    kernel_size=3,\n",
    "\n",
    "    # Local conditioning (set negative value to disable))\n",
    "    cin_channels=80,\n",
    "    cin_pad=0,\n",
    "    # If True, use transposed convolutions to upsample conditional features,\n",
    "    # otherwise repeat features to adjust time resolution\n",
    "    upsample_conditional_features=True,\n",
    "    upsample_net=\"ConvInUpsampleNetwork\",\n",
    "    upsample_params={\n",
    "        \"upsample_scales\": [4, 4, 4, 4],  # should np.prod(upsample_scales) == hop_size\n",
    "    },\n",
    "\n",
    "    # Global conditioning (set negative value to disable)\n",
    "    # currently limited for speaker embedding\n",
    "    # this should only be enabled for multi-speaker dataset\n",
    "    gin_channels=-1,  # i.e., speaker embedding dim\n",
    "    n_speakers=7,  # 7 for CMU ARCTIC\n",
    "\n",
    "    # Data loader\n",
    "    pin_memory=True,\n",
    "    num_workers=1,\n",
    "\n",
    "    # Loss\n",
    "\n",
    "    # Training:\n",
    "    batch_size=8,\n",
    "    optimizer=\"Adam\",\n",
    "    optimizer_params={\n",
    "        \"lr\": 1e-3,\n",
    "        \"eps\": 1e-8,\n",
    "        \"weight_decay\": 0.0,\n",
    "    },\n",
    "\n",
    "    # see lrschedule.py for available lr_schedule\n",
    "    lr_schedule=\"step_learning_rate_decay\",\n",
    "    lr_schedule_kwargs={\"anneal_rate\": 0.5, \"anneal_interval\": 200000},\n",
    "\n",
    "    max_train_steps=1000000,\n",
    "    nepochs=2000,\n",
    "\n",
    "    clip_thresh=-1,\n",
    "\n",
    "    # max time steps can either be specified as sec or steps\n",
    "    # if both are None, then full audio samples are used in a batch\n",
    "    max_time_sec=None,\n",
    "    max_time_steps=10240,  # 256 * 40\n",
    "\n",
    "    # Hold moving averaged parameters and use them for evaluation\n",
    "    exponential_moving_average=True,\n",
    "    # averaged = decay * averaged + (1 - decay) * x\n",
    "    ema_decay=0.9999,\n",
    "\n",
    "    # Save\n",
    "    # per-step intervals\n",
    "    checkpoint_interval=100000,\n",
    "    train_eval_interval=100000,\n",
    "    # per-epoch interval\n",
    "    test_eval_epoch_interval=50,\n",
    "    save_optimizer_state=True,\n",
    "\n",
    "    # Eval:\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def build_model(hparams):\n",
    "    if is_mulaw_quantize(hparams.input_type):\n",
    "        if hparams.out_channels != hparams.quantize_channels:\n",
    "            raise RuntimeError(\n",
    "                \"out_channels must equal to quantize_chennels if input_type is 'mulaw-quantize'\")\n",
    "    if hparams.upsample_conditional_features and hparams.cin_channels < 0:\n",
    "        s = \"Upsample conv layers were specified while local conditioning disabled. \"\n",
    "        s += \"Notice that upsample conv layers will never be used.\"\n",
    "        warn(s)\n",
    "\n",
    "    upsample_params = hparams.upsample_params\n",
    "    upsample_params[\"cin_channels\"] = hparams.cin_channels\n",
    "    upsample_params[\"cin_pad\"] = hparams.cin_pad\n",
    "    model = WaveNet(\n",
    "        out_channels=hparams.out_channels,\n",
    "        layers=hparams.layers,\n",
    "        stacks=hparams.stacks,\n",
    "        residual_channels=hparams.residual_channels,\n",
    "        gate_channels=hparams.gate_channels,\n",
    "        skip_out_channels=hparams.skip_out_channels,\n",
    "        cin_channels=hparams.cin_channels,\n",
    "        gin_channels=hparams.gin_channels,\n",
    "        n_speakers=hparams.n_speakers,\n",
    "        dropout=hparams.dropout,\n",
    "        kernel_size=hparams.kernel_size,\n",
    "        cin_pad=hparams.cin_pad,\n",
    "        upsample_conditional_features=hparams.upsample_conditional_features,\n",
    "        upsample_params=upsample_params,\n",
    "        scalar_input=is_scalar_input(hparams.input_type),\n",
    "        output_distribution=hparams.output_distribution,\n",
    "    )\n",
    "    return model\n",
    "\n",
    "model = build_model(hparams)\n",
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, amsgrad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_files = sorted(glob.glob(\"./data/fma_tiny_16k_15s/raw/*.npz\"))\n",
    "processed_files = sorted(glob.glob(\"./data/fma_tiny_16k_15s/processed/*.npz\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c_batch=torch.Size([100, 80, 188]) x=torch.Size([100, 48128])\n"
     ]
    }
   ],
   "source": [
    "dataset = get_data_loader(processed_files, batch_size=1, shuffle=False, n_split=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbdea58238514f728e9d597cadc4af81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='training', max=1500.0, style=ProgressStyle(description_wiâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3413a581a93942fb90c4f3ca3c9f1954",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='training', max=1500.0, style=ProgressStyle(description_wiâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adbbf4a7f305443192feb0c3f677789a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='training', max=1500.0, style=ProgressStyle(description_wiâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b6c3c74dd2b4f0fac7a2a1302731d63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='training', max=1500.0, style=ProgressStyle(description_wiâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fdb5106d81749ae84430cb2d9a70a67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='training', max=1500.0, style=ProgressStyle(description_wiâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "713ee6ad70c045189ab7c4255da20f38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='training', max=1500.0, style=ProgressStyle(description_wiâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e72eeab27541488c8b81a8f646e64f17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='training', max=1500.0, style=ProgressStyle(description_wiâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bae991022ec432ab72896282f17752b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='training', max=1500.0, style=ProgressStyle(description_wiâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f37b9547dd9b4eac92f5a04a8fba4446",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='training', max=1500.0, style=ProgressStyle(description_wiâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train(dataset, model, optimizer, \n",
    "      device, train_one_step, \n",
    "      epoch_seeds=[1] * 100, global_step=0, n_epoches=15, \n",
    "      no_steps_per_epoch=len(dataset), \n",
    "      ckpt_dir=\"./ckpts\",\n",
    "      log_freq=1, log_metrics=[MeanMetric(x) for x in ['loss', 'accuracy']])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "examples = list(iter(dataset))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pbar = tqdm(dataset)\n",
    "for i in range(200):\n",
    "    for example in examples[:3]:\n",
    "        result = train_one_step(model, optimizer, example, i, device)\n",
    "        result = {k: v.item() for k, v in result.items()}\n",
    "        pbar.update(1)\n",
    "        pbar.set_postfix(**result)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "csci-699-project",
   "language": "python",
   "name": "csci-699-project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
